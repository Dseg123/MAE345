{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference Notebook\n",
    "\n",
    "This notebook loads a trained drone imitation learning model and tests it on sample data.\n",
    "\n",
    "## Setup\n",
    "Update the `EXPERIMENT_DIR` variable below to point to your trained experiment directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from drone.datasets.dataloader import CrazyflieILDataset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Experiment Configuration and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE THIS PATH to your experiment directory\n",
    "EXPERIMENT_DIR = Path(\"/scratch/gpfs/TSILVER/de7281/MAE345/run_20251210_041734\")\n",
    "\n",
    "# Load the training configuration\n",
    "config_path = EXPERIMENT_DIR / \"configs\" / \"train_config_test.yaml\"\n",
    "cfg = OmegaConf.load(config_path)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve paths relative to project root\n",
    "cfg.dataset.data_dir = str(project_root / cfg.dataset.data_dir)\n",
    "\n",
    "# Load model checkpoint\n",
    "model_path = EXPERIMENT_DIR / \"models\" / \"model.pth\"\n",
    "\n",
    "# Instantiate model from config\n",
    "model = instantiate(cfg.models)\n",
    "\n",
    "# Load trained weights\n",
    "state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"\\nModel loaded from: {model_path}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(f\"  Action dim: {model.action_dim}\")\n",
    "print(f\"  Num bins: {model.num_bins}\")\n",
    "print(f\"  Action range: [{model.action_low}, {model.action_high}]\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with same settings as training\n",
    "dataset = CrazyflieILDataset(\n",
    "    data_dir=cfg.dataset.data_dir,\n",
    "    trial_numbers=cfg.training.val_trials,  # Use validation trials or None for all\n",
    "    image_size=cfg.dataset.image_size,\n",
    "    normalize_images=cfg.dataset.normalize_images,\n",
    "    normalize_states=cfg.dataset.normalize_states,\n",
    "    normalize_actions=cfg.dataset.normalize_actions,\n",
    "    augment=False  # No augmentation for testing\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loaded with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 2.5. Analyze Action Distribution (Left vs Right Moves)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze distribution of left vs right moves in the dataset\n# Left move: vy (second coordinate) = +0.2\n# Right move: vy (second coordinate) = -0.2\n\nprint(\"Analyzing action distribution across entire dataset...\")\nprint(f\"Total samples in dataset: {len(dataset)}\\n\")\n\n# Collect all actions from the dataset\nall_actions = []\nfor i in range(len(dataset)):\n    sample = dataset[i]\n    action = sample['action']  # (action_dim,) tensor\n    all_actions.append(action.numpy())\n\nall_actions = np.array(all_actions)  # (N, action_dim) where action_dim=4\n\n# Extract vy (second coordinate, index 1)\nvy_values = all_actions[:, 1]\n\n# Count left and right moves (with some tolerance for floating point)\ntolerance = 0.01\nleft_moves = np.sum(np.abs(vy_values - 0.2) < tolerance)\nright_moves = np.sum(np.abs(vy_values + 0.2) < tolerance)\nother_moves = len(vy_values) - left_moves - right_moves\n\n# Compute fractions\ntotal = len(vy_values)\nleft_fraction = left_moves / total\nright_fraction = right_moves / total\nother_fraction = other_moves / total\n\nprint(\"=\" * 60)\nprint(\"LEFT vs RIGHT MOVE ANALYSIS\")\nprint(\"=\" * 60)\nprint(f\"Left moves  (vy = +0.2): {left_moves:6d} ({left_fraction*100:5.2f}%)\")\nprint(f\"Right moves (vy = -0.2): {right_moves:6d} ({right_fraction*100:5.2f}%)\")\nprint(f\"Other moves:              {other_moves:6d} ({other_fraction*100:5.2f}%)\")\nprint(\"-\" * 60)\nprint(f\"Total:                    {total:6d} (100.00%)\")\nprint(\"=\" * 60)\n\n# Visualize the distribution\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Pie chart\nax1 = axes[0]\ncolors = ['#ff9999', '#66b3ff', '#99ff99']\nlabels = [f'Left (+0.2)\\n{left_fraction*100:.1f}%', \n          f'Right (-0.2)\\n{right_fraction*100:.1f}%', \n          f'Other\\n{other_fraction*100:.1f}%']\nsizes = [left_moves, right_moves, other_moves]\nax1.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\nax1.set_title('Distribution of Left vs Right Moves')\n\n# Histogram of all vy values\nax2 = axes[1]\nax2.hist(vy_values, bins=50, alpha=0.7, edgecolor='black')\nax2.axvline(0.2, color='red', linestyle='--', linewidth=2, label='Left (+0.2)')\nax2.axvline(-0.2, color='blue', linestyle='--', linewidth=2, label='Right (-0.2)')\nax2.axvline(0, color='green', linestyle='--', linewidth=1, label='Straight (0.0)')\nax2.set_xlabel('vy (lateral velocity)')\nax2.set_ylabel('Frequency')\nax2.set_title('Distribution of vy Values')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Show statistics for all action dimensions\nprint(\"\\nAction statistics across all dimensions:\")\nprint(\"=\" * 60)\naction_names = ['vx', 'vy', 'vz', 'yaw_rate']\nfor i, name in enumerate(action_names):\n    values = all_actions[:, i]\n    print(f\"{name}:\")\n    print(f\"  Min:    {values.min():7.4f}\")\n    print(f\"  Max:    {values.max():7.4f}\")\n    print(f\"  Mean:   {values.mean():7.4f}\")\n    print(f\"  Median: {np.median(values):7.4f}\")\n    print(f\"  Std:    {values.std():7.4f}\")\n    print()\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Model on Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(model, image_tensor, device):\n",
    "    \"\"\"Run inference on a single image.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Add batch dimension if needed\n",
    "        if image_tensor.dim() == 3:\n",
    "            image_tensor = image_tensor.unsqueeze(0)\n",
    "        \n",
    "        image_tensor = image_tensor.to(device)\n",
    "        \n",
    "        # Get model output\n",
    "        logits = model(image_tensor)  # (1, action_dim, num_bins)\n",
    "        probs = torch.softmax(logits, dim=-1)  # Convert to probabilities\n",
    "        \n",
    "        # Get continuous action from logits\n",
    "        continuous_action = model.output_to_executable_actions(logits)  # (1, action_dim)\n",
    "        \n",
    "        # Get predicted bin indices\n",
    "        bin_indices = torch.argmax(logits, dim=-1)  # (1, action_dim)\n",
    "        \n",
    "        return {\n",
    "            'logits': logits.cpu(),\n",
    "            'probs': probs.cpu(),\n",
    "            'continuous_action': continuous_action.cpu(),\n",
    "            'bin_indices': bin_indices.cpu()\n",
    "        }\n",
    "\n",
    "def denormalize_image(image_tensor):\n",
    "    \"\"\"Convert normalized image tensor to displayable numpy array.\"\"\"\n",
    "    # Assuming ImageNet normalization was used\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    img = image_tensor.cpu() * std + mean\n",
    "    img = img.clamp(0, 1)\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random samples\n",
    "num_samples = 5\n",
    "random_indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(num_samples, 2, figsize=(15, 4*num_samples))\n",
    "if num_samples == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "action_names = ['vx', 'vy', 'vz', 'yaw_rate']\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    # Get sample\n",
    "    sample = dataset[idx]\n",
    "    image = sample['observation']\n",
    "    true_action = sample['action']\n",
    "    state = sample['state']\n",
    "    \n",
    "    # Predict action\n",
    "    pred_output = predict_action(model, image, device)\n",
    "    pred_action = pred_output['continuous_action'].squeeze(0)  # (action_dim,)\n",
    "    probs = pred_output['probs'].squeeze(0)  # (action_dim, num_bins)\n",
    "    \n",
    "    # Display image\n",
    "    ax_img = axes[i, 0]\n",
    "    display_img = denormalize_image(image)\n",
    "    ax_img.imshow(display_img)\n",
    "    ax_img.set_title(f\"Sample {idx}\\nState: [{state[0]:.2f}, {state[1]:.2f}, {state[2]:.2f}]\")\n",
    "    ax_img.axis('off')\n",
    "    \n",
    "    # Display action comparison\n",
    "    ax_action = axes[i, 1]\n",
    "    x = np.arange(len(action_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax_action.bar(x - width/2, true_action.numpy(), width, label='Ground Truth', alpha=0.8)\n",
    "    ax_action.bar(x + width/2, pred_action.numpy(), width, label='Predicted', alpha=0.8)\n",
    "    \n",
    "    ax_action.set_ylabel('Action Value')\n",
    "    ax_action.set_title('Action Comparison')\n",
    "    ax_action.set_xticks(x)\n",
    "    ax_action.set_xticklabels(action_names)\n",
    "    ax_action.legend()\n",
    "    ax_action.grid(True, alpha=0.3)\n",
    "    ax_action.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    # Add error text\n",
    "    error = torch.abs(pred_action - true_action).mean()\n",
    "    ax_action.text(0.02, 0.98, f'MAE: {error:.4f}', \n",
    "                   transform=ax_action.transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Probability Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one sample to analyze in detail\n",
    "sample_idx = random_indices[0]\n",
    "sample = dataset[sample_idx]\n",
    "image = sample['observation']\n",
    "true_action = sample['action']\n",
    "\n",
    "# Get prediction\n",
    "pred_output = predict_action(model, image, device)\n",
    "probs = pred_output['probs'].squeeze(0)  # (action_dim, num_bins)\n",
    "pred_action = pred_output['continuous_action'].squeeze(0)\n",
    "\n",
    "# Convert true action to bins for visualization\n",
    "true_bins = model.continuous_to_bins(true_action.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "# Create bin values for x-axis\n",
    "bin_values = np.linspace(model.action_low, model.action_high, model.num_bins)\n",
    "\n",
    "# Plot probability distributions for each action dimension\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, action_name in enumerate(action_names):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot probability distribution\n",
    "    ax.bar(bin_values, probs[i].numpy(), width=(bin_values[1]-bin_values[0])*0.8, \n",
    "           alpha=0.6, label='Predicted Probability')\n",
    "    \n",
    "    # Mark predicted action\n",
    "    ax.axvline(pred_action[i].item(), color='blue', linestyle='--', \n",
    "               linewidth=2, label=f'Predicted: {pred_action[i]:.3f}')\n",
    "    \n",
    "    # Mark true action\n",
    "    ax.axvline(true_action[i].item(), color='red', linestyle='--', \n",
    "               linewidth=2, label=f'True: {true_action[i]:.3f}')\n",
    "    \n",
    "    ax.set_xlabel('Action Value')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_title(f'{action_name} Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(model.action_low - 0.05, model.action_high + 0.05)\n",
    "\n",
    "plt.suptitle(f'Action Probability Distributions - Sample {sample_idx}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the image for this sample\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(denormalize_image(image))\n",
    "plt.title(f'Input Image - Sample {sample_idx}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Overall Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on a larger subset\n",
    "num_eval_samples = min(100, len(dataset))\n",
    "eval_indices = np.random.choice(len(dataset), num_eval_samples, replace=False)\n",
    "\n",
    "all_true_actions = []\n",
    "all_pred_actions = []\n",
    "\n",
    "print(f\"Evaluating on {num_eval_samples} samples...\")\n",
    "for idx in eval_indices:\n",
    "    sample = dataset[idx]\n",
    "    image = sample['observation']\n",
    "    true_action = sample['action']\n",
    "    \n",
    "    pred_output = predict_action(model, image, device)\n",
    "    pred_action = pred_output['continuous_action'].squeeze(0)\n",
    "    \n",
    "    all_true_actions.append(true_action.numpy())\n",
    "    all_pred_actions.append(pred_action.numpy())\n",
    "\n",
    "all_true_actions = np.array(all_true_actions)  # (N, action_dim)\n",
    "all_pred_actions = np.array(all_pred_actions)  # (N, action_dim)\n",
    "\n",
    "# Compute metrics\n",
    "mae = np.abs(all_true_actions - all_pred_actions).mean(axis=0)\n",
    "mse = ((all_true_actions - all_pred_actions)**2).mean(axis=0)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Evaluation Results on {num_eval_samples} samples\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Action':<12} {'MAE':<12} {'RMSE':<12}\")\n",
    "print(\"-\"*60)\n",
    "for i, action_name in enumerate(action_names):\n",
    "    print(f\"{action_name:<12} {mae[i]:<12.4f} {rmse[i]:<12.4f}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Overall':<12} {mae.mean():<12.4f} {rmse.mean():<12.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error distributions\n",
    "errors = all_pred_actions - all_true_actions\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, action_name in enumerate(action_names):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Histogram of errors\n",
    "    ax.hist(errors[:, i], bins=30, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "    ax.axvline(errors[:, i].mean(), color='green', linestyle='--', \n",
    "               linewidth=2, label=f'Mean: {errors[:, i].mean():.4f}')\n",
    "    \n",
    "    ax.set_xlabel('Prediction Error')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{action_name} Error Distribution\\nStd: {errors[:, i].std():.4f}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Prediction Error Distributions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on Custom Image (Optional)\n",
    "\n",
    "Load your own image and see what action the model predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_image_path(image_path, model, device):\n",
    "    \"\"\"Load an image from disk and predict action.\"\"\"\n",
    "    # Load and preprocess image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Apply same transforms as dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(cfg.dataset.image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    img_tensor = transform(img)\n",
    "    \n",
    "    # Predict\n",
    "    pred_output = predict_action(model, img_tensor, device)\n",
    "    pred_action = pred_output['continuous_action'].squeeze(0)\n",
    "    probs = pred_output['probs'].squeeze(0)\n",
    "    \n",
    "    # Display\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Show image\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title('Input Image')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Show predicted action\n",
    "    ax2.bar(action_names, pred_action.numpy())\n",
    "    ax2.set_ylabel('Action Value')\n",
    "    ax2.set_title('Predicted Action')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Predicted action:\")\n",
    "    for name, val in zip(action_names, pred_action.numpy()):\n",
    "        print(f\"  {name}: {val:.4f}\")\n",
    "    \n",
    "    return pred_action, probs\n",
    "\n",
    "# Example usage (uncomment and provide your own image path):\n",
    "# custom_image_path = \"path/to/your/image.png\"\n",
    "# pred_action, probs = predict_from_image_path(custom_image_path, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Predictions (Optional)\n",
    "\n",
    "Save predictions to a file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "results = {\n",
    "    'experiment_dir': str(EXPERIMENT_DIR),\n",
    "    'model_path': str(model_path),\n",
    "    'num_samples_evaluated': num_eval_samples,\n",
    "    'metrics': {\n",
    "        'mae_per_action': {name: float(mae[i]) for i, name in enumerate(action_names)},\n",
    "        'rmse_per_action': {name: float(rmse[i]) for i, name in enumerate(action_names)},\n",
    "        'overall_mae': float(mae.mean()),\n",
    "        'overall_rmse': float(rmse.mean())\n",
    "    },\n",
    "    'config': OmegaConf.to_container(cfg, resolve=True)\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "output_path = EXPERIMENT_DIR / \"inference_results.json\"\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}