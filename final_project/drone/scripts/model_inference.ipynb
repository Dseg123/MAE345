{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference Notebook\n",
    "\n",
    "This notebook loads a trained drone imitation learning model and tests it on sample data.\n",
    "\n",
    "## Setup\n",
    "Update the `EXPERIMENT_DIR` variable below to point to your trained experiment directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu130\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from drone.datasets.dataloader import CrazyflieILDataset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Experiment Configuration and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "models:\n",
      "  _target_: drone.models.discrete_action_model.DiscreteActionModel\n",
      "  pretrained: false\n",
      "  action_dim: 4\n",
      "  num_bins: 11\n",
      "  action_low: -0.2\n",
      "  action_high: 0.2\n",
      "  output_space: discrete\n",
      "  v3: false\n",
      "experiment_dir: /scratch/gpfs/TSILVER/de7281/MAE345/job_2910637\n",
      "dataset:\n",
      "  data_dir: drone/datasets/imitation_data\n",
      "  image_size:\n",
      "  - 224\n",
      "  - 224\n",
      "  augment: true\n",
      "  normalize_states: false\n",
      "  normalize_images: true\n",
      "  normalize_actions: false\n",
      "training:\n",
      "  train_trials: null\n",
      "  val_trials: null\n",
      "  batch_size: 32\n",
      "  num_epochs: 20\n",
      "  lr: 0.005\n",
      "  weight_decay: 0.001\n",
      "  early_stopping_patience: 1\n",
      "  early_stopping_min_delta: 0.001\n",
      "  num_workers: 8\n",
      "  shuffle_train: true\n",
      "log_dir: recordings\n",
      "group_number: 5\n",
      "camera_id: 0\n",
      "hover_height: 0.5\n",
      "fly_steps: 100\n",
      "crop_top: 80\n",
      "ckpt_path: drone/trained_models/model.pth\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UPDATE THIS PATH to your experiment directory\n",
    "EXPERIMENT_DIR = Path(\"/scratch/gpfs/TSILVER/de7281/MAE345/job_2910637\")\n",
    "\n",
    "# Load the training configuration\n",
    "config_path = EXPERIMENT_DIR / \"configs\" / \"config.yaml\"\n",
    "cfg = OmegaConf.load(config_path)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "InstantiationException",
     "evalue": "Error in call to target 'drone.models.discrete_action_model.DiscreteActionModel':\nTypeError(\"DiscreteActionModel.__init__() got an unexpected keyword argument 'output_space'\")\nfull_key: models",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mae345/lib/python3.11/site-packages/hydra/_internal/instantiate/_instantiate2.py:92\u001b[39m, in \u001b[36m_call_target\u001b[39m\u001b[34m(_target_, _partial_, args, kwargs, full_key)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_target_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mTypeError\u001b[39m: DiscreteActionModel.__init__() got an unexpected keyword argument 'output_space'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mInstantiationException\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m model_path = EXPERIMENT_DIR / \u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mmodel.pth\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Instantiate model from config\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m model = \u001b[43minstantiate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Load trained weights\u001b[39;00m\n\u001b[32m     11\u001b[39m state_dict = torch.load(model_path, map_location=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mae345/lib/python3.11/site-packages/hydra/_internal/instantiate/_instantiate2.py:226\u001b[39m, in \u001b[36minstantiate\u001b[39m\u001b[34m(config, *args, **kwargs)\u001b[39m\n\u001b[32m    223\u001b[39m     _convert_ = config.pop(_Keys.CONVERT, ConvertMode.NONE)\n\u001b[32m    224\u001b[39m     _partial_ = config.pop(_Keys.PARTIAL, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minstantiate_node\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_recursive_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_convert_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_partial_\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m OmegaConf.is_list(config):\n\u001b[32m    230\u001b[39m     \u001b[38;5;66;03m# Finalize config (convert targets to strings, merge with kwargs)\u001b[39;00m\n\u001b[32m    231\u001b[39m     config_copy = copy.deepcopy(config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mae345/lib/python3.11/site-packages/hydra/_internal/instantiate/_instantiate2.py:347\u001b[39m, in \u001b[36minstantiate_node\u001b[39m\u001b[34m(node, convert, recursive, partial, *args)\u001b[39m\n\u001b[32m    342\u001b[39m                 value = instantiate_node(\n\u001b[32m    343\u001b[39m                     value, convert=convert, recursive=recursive\n\u001b[32m    344\u001b[39m                 )\n\u001b[32m    345\u001b[39m             kwargs[key] = _convert_node(value, convert)\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_target_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    349\u001b[39m     \u001b[38;5;66;03m# If ALL or PARTIAL non structured or OBJECT non structured,\u001b[39;00m\n\u001b[32m    350\u001b[39m     \u001b[38;5;66;03m# instantiate in dict and resolve interpolations eagerly.\u001b[39;00m\n\u001b[32m    351\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert == ConvertMode.ALL \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    352\u001b[39m         convert \u001b[38;5;129;01min\u001b[39;00m (ConvertMode.PARTIAL, ConvertMode.OBJECT)\n\u001b[32m    353\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m node._metadata.object_type \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m    354\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mae345/lib/python3.11/site-packages/hydra/_internal/instantiate/_instantiate2.py:97\u001b[39m, in \u001b[36m_call_target\u001b[39m\u001b[34m(_target_, _partial_, args, kwargs, full_key)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m full_key:\n\u001b[32m     96\u001b[39m     msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mfull_key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m InstantiationException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mInstantiationException\u001b[39m: Error in call to target 'drone.models.discrete_action_model.DiscreteActionModel':\nTypeError(\"DiscreteActionModel.__init__() got an unexpected keyword argument 'output_space'\")\nfull_key: models"
     ]
    }
   ],
   "source": [
    "# Resolve paths relative to project root\n",
    "cfg.dataset.data_dir = str(project_root / cfg.dataset.data_dir)\n",
    "\n",
    "# Load model checkpoint\n",
    "model_path = EXPERIMENT_DIR / \"models\" / \"model.pth\"\n",
    "\n",
    "# Instantiate model from config\n",
    "model_cfg = OmegaConf.to_container(cfg.models, resolve=True)\n",
    "output_space = model_cfg.pop('output_space', None)  # Remove before instantiation\n",
    "model = instantiate(model_cfg)\n",
    "\n",
    "# Load trained weights\n",
    "state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"\\nModel loaded from: {model_path}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(f\"  Action dim: {model.action_dim}\")\n",
    "print(f\"  Num bins: {model.num_bins}\")\n",
    "print(f\"  Action range: [{model.action_low}, {model.action_high}]\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with same settings as training\n",
    "dataset = CrazyflieILDataset(\n",
    "    data_dir=cfg.dataset.data_dir,\n",
    "    trial_numbers=cfg.training.val_trials,  # Use validation trials or None for all\n",
    "    image_size=cfg.dataset.image_size,\n",
    "    normalize_images=cfg.dataset.normalize_images,\n",
    "    normalize_states=cfg.dataset.normalize_states,\n",
    "    normalize_actions=cfg.dataset.normalize_actions,\n",
    "    augment=False  # No augmentation for testing\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loaded with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Analyze Action Distribution (Left vs Right Moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distribution of left vs right moves in the dataset\n",
    "# Left move: vy (second coordinate) = +0.2\n",
    "# Right move: vy (second coordinate) = -0.2\n",
    "\n",
    "print(\"Analyzing action distribution across entire dataset...\")\n",
    "print(f\"Total samples in dataset: {len(dataset)}\\n\")\n",
    "\n",
    "# Collect all actions from the dataset\n",
    "all_actions = []\n",
    "for i in range(len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    action = sample['action']  # (action_dim,) tensor\n",
    "    all_actions.append(action.numpy())\n",
    "\n",
    "all_actions = np.array(all_actions)  # (N, action_dim) where action_dim=4\n",
    "\n",
    "# Extract vy (second coordinate, index 1)\n",
    "vy_values = all_actions[:, 1]\n",
    "\n",
    "# Count left and right moves (with some tolerance for floating point)\n",
    "tolerance = 0.01\n",
    "left_moves = np.sum(np.abs(vy_values - 0.2) < tolerance)\n",
    "right_moves = np.sum(np.abs(vy_values + 0.2) < tolerance)\n",
    "other_moves = len(vy_values) - left_moves - right_moves\n",
    "\n",
    "# Compute fractions\n",
    "total = len(vy_values)\n",
    "left_fraction = left_moves / total\n",
    "right_fraction = right_moves / total\n",
    "other_fraction = other_moves / total\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LEFT vs RIGHT MOVE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Left moves  (vy = +0.2): {left_moves:6d} ({left_fraction*100:5.2f}%)\")\n",
    "print(f\"Right moves (vy = -0.2): {right_moves:6d} ({right_fraction*100:5.2f}%)\")\n",
    "print(f\"Other moves:              {other_moves:6d} ({other_fraction*100:5.2f}%)\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total:                    {total:6d} (100.00%)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Visualize the distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "ax1 = axes[0]\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "labels = [f'Left (+0.2)\\n{left_fraction*100:.1f}%', \n",
    "          f'Right (-0.2)\\n{right_fraction*100:.1f}%', \n",
    "          f'Other\\n{other_fraction*100:.1f}%']\n",
    "sizes = [left_moves, right_moves, other_moves]\n",
    "ax1.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Distribution of Left vs Right Moves')\n",
    "\n",
    "# Histogram of all vy values\n",
    "ax2 = axes[1]\n",
    "ax2.hist(vy_values, bins=50, alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(0.2, color='red', linestyle='--', linewidth=2, label='Left (+0.2)')\n",
    "ax2.axvline(-0.2, color='blue', linestyle='--', linewidth=2, label='Right (-0.2)')\n",
    "ax2.axvline(0, color='green', linestyle='--', linewidth=1, label='Straight (0.0)')\n",
    "ax2.set_xlabel('vy (lateral velocity)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Distribution of vy Values')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show statistics for all action dimensions\n",
    "print(\"\\nAction statistics across all dimensions:\")\n",
    "print(\"=\" * 60)\n",
    "action_names = ['vx', 'vy', 'vz', 'yaw_rate']\n",
    "for i, name in enumerate(action_names):\n",
    "    values = all_actions[:, i]\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Min:    {values.min():7.4f}\")\n",
    "    print(f\"  Max:    {values.max():7.4f}\")\n",
    "    print(f\"  Mean:   {values.mean():7.4f}\")\n",
    "    print(f\"  Median: {np.median(values):7.4f}\")\n",
    "    print(f\"  Std:    {values.std():7.4f}\")\n",
    "    print()\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Model on Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(model, image_tensor, device):\n",
    "    \"\"\"Run inference on a single image.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Add batch dimension if needed\n",
    "        if image_tensor.dim() == 3:\n",
    "            image_tensor = image_tensor.unsqueeze(0)\n",
    "        \n",
    "        image_tensor = image_tensor.to(device)\n",
    "        \n",
    "        # Get model output\n",
    "        logits = model(image_tensor)  # (1, action_dim, num_bins)\n",
    "        probs = torch.softmax(logits, dim=-1)  # Convert to probabilities\n",
    "        \n",
    "        # Get continuous action from logits\n",
    "        continuous_action = model.output_to_executable_actions(logits)  # (1, action_dim)\n",
    "        \n",
    "        # Get predicted bin indices\n",
    "        bin_indices = torch.argmax(logits, dim=-1)  # (1, action_dim)\n",
    "        \n",
    "        return {\n",
    "            'logits': logits.cpu(),\n",
    "            'probs': probs.cpu(),\n",
    "            'continuous_action': continuous_action.cpu(),\n",
    "            'bin_indices': bin_indices.cpu()\n",
    "        }\n",
    "\n",
    "def denormalize_image(image_tensor):\n",
    "    \"\"\"Convert normalized image tensor to displayable numpy array.\"\"\"\n",
    "    # Assuming ImageNet normalization was used\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    img = image_tensor.cpu() * std + mean\n",
    "    img = img.clamp(0, 1)\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random samples\n",
    "num_samples = 5\n",
    "random_indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(num_samples, 2, figsize=(15, 4*num_samples))\n",
    "if num_samples == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "action_names = ['vx', 'vy', 'vz', 'yaw_rate']\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    # Get sample\n",
    "    sample = dataset[idx]\n",
    "    image = sample['observation']\n",
    "    true_action = sample['action']\n",
    "    state = sample['state']\n",
    "    \n",
    "    # Predict action\n",
    "    pred_output = predict_action(model, image, device)\n",
    "    pred_action = pred_output['continuous_action'].squeeze(0)  # (action_dim,)\n",
    "    probs = pred_output['probs'].squeeze(0)  # (action_dim, num_bins)\n",
    "    \n",
    "    # Display image\n",
    "    ax_img = axes[i, 0]\n",
    "    display_img = denormalize_image(image)\n",
    "    ax_img.imshow(display_img)\n",
    "    ax_img.set_title(f\"Sample {idx}\\nState: [{state[0]:.2f}, {state[1]:.2f}, {state[2]:.2f}]\")\n",
    "    ax_img.axis('off')\n",
    "    \n",
    "    # Display action comparison\n",
    "    ax_action = axes[i, 1]\n",
    "    x = np.arange(len(action_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax_action.bar(x - width/2, true_action.numpy(), width, label='Ground Truth', alpha=0.8)\n",
    "    ax_action.bar(x + width/2, pred_action.numpy(), width, label='Predicted', alpha=0.8)\n",
    "    \n",
    "    ax_action.set_ylabel('Action Value')\n",
    "    ax_action.set_title('Action Comparison')\n",
    "    ax_action.set_xticks(x)\n",
    "    ax_action.set_xticklabels(action_names)\n",
    "    ax_action.legend()\n",
    "    ax_action.grid(True, alpha=0.3)\n",
    "    ax_action.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    # Add error text\n",
    "    error = torch.abs(pred_action - true_action).mean()\n",
    "    ax_action.text(0.02, 0.98, f'MAE: {error:.4f}', \n",
    "                   transform=ax_action.transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Probability Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one sample to analyze in detail\n",
    "sample_idx = random_indices[0]\n",
    "sample = dataset[sample_idx]\n",
    "image = sample['observation']\n",
    "true_action = sample['action']\n",
    "\n",
    "# Get prediction\n",
    "pred_output = predict_action(model, image, device)\n",
    "probs = pred_output['probs'].squeeze(0)  # (action_dim, num_bins)\n",
    "pred_action = pred_output['continuous_action'].squeeze(0)\n",
    "\n",
    "# Convert true action to bins for visualization\n",
    "true_bins = model.continuous_to_bins(true_action.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "# Create bin values for x-axis\n",
    "bin_values = np.linspace(model.action_low, model.action_high, model.num_bins)\n",
    "\n",
    "# Plot probability distributions for each action dimension\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, action_name in enumerate(action_names):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot probability distribution\n",
    "    ax.bar(bin_values, probs[i].numpy(), width=(bin_values[1]-bin_values[0])*0.8, \n",
    "           alpha=0.6, label='Predicted Probability')\n",
    "    \n",
    "    # Mark predicted action\n",
    "    ax.axvline(pred_action[i].item(), color='blue', linestyle='--', \n",
    "               linewidth=2, label=f'Predicted: {pred_action[i]:.3f}')\n",
    "    \n",
    "    # Mark true action\n",
    "    ax.axvline(true_action[i].item(), color='red', linestyle='--', \n",
    "               linewidth=2, label=f'True: {true_action[i]:.3f}')\n",
    "    \n",
    "    ax.set_xlabel('Action Value')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_title(f'{action_name} Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(model.action_low - 0.05, model.action_high + 0.05)\n",
    "\n",
    "plt.suptitle(f'Action Probability Distributions - Sample {sample_idx}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the image for this sample\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(denormalize_image(image))\n",
    "plt.title(f'Input Image - Sample {sample_idx}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Overall Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on a larger subset\n",
    "num_eval_samples = min(100, len(dataset))\n",
    "eval_indices = np.random.choice(len(dataset), num_eval_samples, replace=False)\n",
    "\n",
    "all_true_actions = []\n",
    "all_pred_actions = []\n",
    "\n",
    "print(f\"Evaluating on {num_eval_samples} samples...\")\n",
    "for idx in eval_indices:\n",
    "    sample = dataset[idx]\n",
    "    image = sample['observation']\n",
    "    true_action = sample['action']\n",
    "    \n",
    "    pred_output = predict_action(model, image, device)\n",
    "    pred_action = pred_output['continuous_action'].squeeze(0)\n",
    "    \n",
    "    all_true_actions.append(true_action.numpy())\n",
    "    all_pred_actions.append(pred_action.numpy())\n",
    "\n",
    "all_true_actions = np.array(all_true_actions)  # (N, action_dim)\n",
    "all_pred_actions = np.array(all_pred_actions)  # (N, action_dim)\n",
    "\n",
    "# Compute metrics\n",
    "mae = np.abs(all_true_actions - all_pred_actions).mean(axis=0)\n",
    "mse = ((all_true_actions - all_pred_actions)**2).mean(axis=0)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Evaluation Results on {num_eval_samples} samples\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Action':<12} {'MAE':<12} {'RMSE':<12}\")\n",
    "print(\"-\"*60)\n",
    "for i, action_name in enumerate(action_names):\n",
    "    print(f\"{action_name:<12} {mae[i]:<12.4f} {rmse[i]:<12.4f}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Overall':<12} {mae.mean():<12.4f} {rmse.mean():<12.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error distributions\n",
    "errors = all_pred_actions - all_true_actions\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, action_name in enumerate(action_names):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Histogram of errors\n",
    "    ax.hist(errors[:, i], bins=30, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "    ax.axvline(errors[:, i].mean(), color='green', linestyle='--', \n",
    "               linewidth=2, label=f'Mean: {errors[:, i].mean():.4f}')\n",
    "    \n",
    "    ax.set_xlabel('Prediction Error')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{action_name} Error Distribution\\nStd: {errors[:, i].std():.4f}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Prediction Error Distributions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on Custom Image (Optional)\n",
    "\n",
    "Load your own image and see what action the model predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_image_path(image_path, model, device):\n",
    "    \"\"\"Load an image from disk and predict action.\"\"\"\n",
    "    # Load and preprocess image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Apply same transforms as dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(cfg.dataset.image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    img_tensor = transform(img)\n",
    "    \n",
    "    # Predict\n",
    "    pred_output = predict_action(model, img_tensor, device)\n",
    "    pred_action = pred_output['continuous_action'].squeeze(0)\n",
    "    probs = pred_output['probs'].squeeze(0)\n",
    "    \n",
    "    # Display\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Show image\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title('Input Image')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Show predicted action\n",
    "    ax2.bar(action_names, pred_action.numpy())\n",
    "    ax2.set_ylabel('Action Value')\n",
    "    ax2.set_title('Predicted Action')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Predicted action:\")\n",
    "    for name, val in zip(action_names, pred_action.numpy()):\n",
    "        print(f\"  {name}: {val:.4f}\")\n",
    "    \n",
    "    return pred_action, probs\n",
    "\n",
    "# Example usage (uncomment and provide your own image path):\n",
    "# custom_image_path = \"path/to/your/image.png\"\n",
    "# pred_action, probs = predict_from_image_path(custom_image_path, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Predictions (Optional)\n",
    "\n",
    "Save predictions to a file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "results = {\n",
    "    'experiment_dir': str(EXPERIMENT_DIR),\n",
    "    'model_path': str(model_path),\n",
    "    'num_samples_evaluated': num_eval_samples,\n",
    "    'metrics': {\n",
    "        'mae_per_action': {name: float(mae[i]) for i, name in enumerate(action_names)},\n",
    "        'rmse_per_action': {name: float(rmse[i]) for i, name in enumerate(action_names)},\n",
    "        'overall_mae': float(mae.mean()),\n",
    "        'overall_rmse': float(rmse.mean())\n",
    "    },\n",
    "    'config': OmegaConf.to_container(cfg, resolve=True)\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "output_path = EXPERIMENT_DIR / \"inference_results.json\"\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mae345",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
